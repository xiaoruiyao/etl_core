"""
FDS Curve Database Parser (ä½¿ç”¨ SQLAlchemy)
è§£æPostgreSQLæ•°æ®åº“ä¸­çš„æ›²çº¿æ•°æ®å¹¶å­˜å‚¨åˆ°curve_plainå­—æ®µ

Database: 10.18.120.240:35432
Database Name: miot_quality_data
Table: origin.fds_curves

ä¾èµ–: 
  - sqlalchemy (å·²å®‰è£… âœ“)
  - psycopg2-binary æˆ– pg8000 (éœ€è¦å®‰è£…å…¶ä¸­ä¹‹ä¸€)

å®‰è£…é©±åŠ¨:
  pip install psycopg2-binary  # æ¨è
  æˆ–
  pip install pg8000  # çº¯Pythonï¼Œæ— éœ€ç¼–è¯‘
"""

import struct
import base64
import json
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta


def parse_fds_curve(curve_data, start_offset=7816):
    """
    è§£æFDSæ›²çº¿æ•°æ®ï¼ˆæ”¯æŒbytesæˆ–Base64å­—ç¬¦ä¸²ï¼‰
    
    Args:
        curve_data: äºŒè¿›åˆ¶æ•°æ®(bytes) æˆ– Base64å­—ç¬¦ä¸²(str)
        start_offset: æ•°æ®èµ·å§‹åç§»é‡ (é»˜è®¤7816)
        
    Returns:
        list: è§£æåçš„è®°å½•åˆ—è¡¨
    """
    if curve_data is None:
        return None

    # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
    if isinstance(curve_data, bytes):
        # ç›´æ¥ä½¿ç”¨äºŒè¿›åˆ¶æ•°æ®ï¼ˆæ¥è‡ªæ•°æ®åº“byteaå­—æ®µï¼‰
        data = curve_data
    elif isinstance(curve_data, str):
        # Base64å­—ç¬¦ä¸²ï¼ˆæ¥è‡ªCSVæ–‡ä»¶ï¼‰
        try:
            data = base64.b64decode(curve_data)
        except Exception as e:
            print(f"Base64è§£ç é”™è¯¯: {e}")
            return None
    else:
        print(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {type(curve_data)}")
        return None

    frame_size = 40
    if len(data) <= start_offset:
        return []

    num_rows = (len(data) - start_offset) // frame_size
    
    curve_records = []
    for i in range(num_rows):
        offset = start_offset + i * frame_size
        chunk = data[offset : offset + frame_size]
        
        try:
            # Variant B ç»“æ„: <hhffffffffi
            values = struct.unpack('<hhffffffffi', chunk)
            
            record = {
                'è½¬é€Ÿè®¾å®šå€¼': values[0],
                'å®é™…è½¬é€Ÿ': values[1],
                'æ‰­çŸ©': round(values[2], 4),
                'è¿‡æ»¤æ‰­çŸ©': round(values[3], 4),
                'æ‰­çŸ©å•ä½è§’åº¦å˜åŒ–é‡': round(values[4], 4),
                'æ·±åº¦': round(values[5], 4),
                'æ·±åº¦å•ä½æ—¶é—´å˜åŒ–é‡': round(values[6], 4),
                'è§’åº¦': round(values[7], 4),
                'å‹åŠ›è®¾å®šå€¼': round(values[8], 4),
                'å®é™…å‹åŠ›': round(values[9], 4),
                'æ­¥éª¤': values[10]
            }
            curve_records.append(record)
        except Exception:
            pass
            
    return curve_records


def create_db_engine(host, port, database, user, password, driver='psycopg2'):
    """
    åˆ›å»ºSQLAlchemyæ•°æ®åº“å¼•æ“
    
    Args:
        driver: 'psycopg2' æˆ– 'pg8000'
    """
    # æ„å»ºè¿æ¥å­—ç¬¦ä¸²
    # postgresql+psycopg2://user:password@host:port/database
    # postgresql+pg8000://user:password@host:port/database
    
    connection_string = f"postgresql+{driver}://{user}:{password}@{host}:{port}/{database}"
    
    try:
        engine = create_engine(connection_string, echo=False)
        # æµ‹è¯•è¿æ¥
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        print(f"âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ (é©±åŠ¨: {driver})")
        return engine
    except Exception as e:
        print(f"âœ— æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        if driver == 'psycopg2':
            print("\næç¤º: å¦‚æœpsycopg2æœªå®‰è£…ï¼Œè¯·å°è¯•:")
            print("  1. pip install psycopg2-binary")
            print("  2. æˆ–ä½¿ç”¨ pg8000: ä¿®æ”¹è„šæœ¬ä¸­çš„ driver='pg8000'")
        raise


def process_curves(engine, batch_size=100, start_date='2024-12-01', min_autoindex=None):
    """
    å¤„ç†æ›²çº¿æ•°æ®
    
    Args:
        engine: SQLAlchemyå¼•æ“
        batch_size: æ‰¹å¤„ç†å¤§å°
        start_date: èµ·å§‹æ—¥æœŸ
        min_autoindex: æœ€å°autoindexå€¼ (å¯é€‰ï¼Œç”¨äºç­›é€‰ autoindex > min_autoindex)
    """
    with engine.connect() as conn:
        # æ„å»ºWHEREæ¡ä»¶
        where_conditions = ["starttime >= :start_date", "curve IS NOT NULL", "curve_plain IS NULL"]
        params = {"start_date": start_date}
        
        if min_autoindex is not None:
            where_conditions.append("autoindex > :min_autoindex")
            params["min_autoindex"] = min_autoindex
        
        where_clause = " AND ".join(where_conditions)
        
        # ç»Ÿè®¡å¾…å¤„ç†è®°å½•æ•°
        count_query = text(f"""
            SELECT COUNT(*) as total 
            FROM origin.fds_curves 
            WHERE {where_clause}
        """)
        
        result = conn.execute(count_query, params)
        total_count = result.scalar()
        
        print(f"\nğŸ“Š å¾…å¤„ç†è®°å½•æ€»æ•°: {total_count}")
        if min_autoindex is not None:
            print(f"   ç­›é€‰æ¡ä»¶: autoindex > {min_autoindex}")
        
        if total_count == 0:
            print("æ²¡æœ‰éœ€è¦å¤„ç†çš„è®°å½•")
            return
        
        # åˆ†æ‰¹å¤„ç†
        offset = 0
        processed = 0
        success = 0
        failed = 0
        
        while True:
            # æŸ¥è¯¢ä¸€æ‰¹æ•°æ® - Always fetch from start since we are removing items from the result set (by updating curve_plain)
            select_query = text(f"""
                SELECT autoindex, curve, starttime, duration, actualprogramid, systemid, startselection, ok_nok_code, lastexecutedstep, cyclenumber, bsn, progselection, create_time, update_time
                FROM origin.fds_curves 
                WHERE {where_clause}
                ORDER BY autoindex
                LIMIT :batch_size
            """)
            
            query_params = params.copy()
            query_params["batch_size"] = batch_size
            
            result = conn.execute(select_query, query_params)
            records = result.fetchall()
            
            if not records:
                break
            
            print(f"\nğŸ”„ å¤„ç†æ‰¹æ¬¡: {processed + 1} åˆ° {processed + len(records)} / {total_count} (å‰©ä½™: {total_count - processed})")
            
            # å¤„ç†æ¯æ¡è®°å½•
            for record in records:
                record_id = record[0]
                curve_data = record[1]
                starttime = record[2]
                duration = record[3]
                
                # Metadata fields
                metadata = {
                    'actualprogramid': record[4],
                    'systemid': record[5],
                    'startselection': record[6],
                    'ok_nok_code': record[7],
                    'lastexecutedstep': record[8],
                    'cyclenumber': record[9],
                    'bsn': record[10],
                    'progselection': record[11],
                    'create_time': record[12],
                    'update_time': record[13]
                }
                
                try:
                    # è§£ææ›²çº¿
                    parsed_data = parse_fds_curve(curve_data)
                    
                    if parsed_data is None:
                        print(f"  âš  ID {record_id}: Base64è§£ç å¤±è´¥")
                        failed += 1
                        continue
                    
                    if len(parsed_data) == 0:
                        print(f"  âš  ID {record_id}: æœªæå–åˆ°æ•°æ®ç‚¹")
                        failed += 1
                        continue
                    
                    # 1. Update curve_plain in origin.fds_curves
                    json_data = json.dumps(parsed_data, ensure_ascii=False)
                    update_query = text("""
                        UPDATE origin.fds_curves 
                        SET curve_plain = CAST(:json_data AS jsonb)
                        WHERE autoindex = :record_id
                    """)
                    conn.execute(
                        update_query, 
                        {"json_data": json_data, "record_id": record_id}
                    )

                    # 2. Split by step and insert into origin.fds_curves_by_step
                    # Group points by step
                    steps_data = {}
                    for point in parsed_data:
                        step_num = point['æ­¥éª¤']
                        if step_num not in steps_data:
                            steps_data[step_num] = []
                        steps_data[step_num].append(point)
                    
                    # Calculate timing
                    total_points = len(parsed_data)
                    time_per_point = 0
                    if total_points > 0 and duration is not None:
                        # Duration is typically in seconds, double check unit if needed. Assuming seconds based on context.
                        time_per_point = float(duration) / total_points
                    
                    current_point_index = 0
                    
                    for step_num in sorted(steps_data.keys()):
                        points = steps_data[step_num]
                        step_points_count = len(points)
                        
                        # Calculate start and end time for this step
                        # Start time offset from curve start
                        start_offset_seconds = current_point_index * time_per_point
                        end_offset_seconds = (current_point_index + step_points_count) * time_per_point
                        
                        step_start_time = starttime + timedelta(seconds=start_offset_seconds)
                        step_end_time = starttime + timedelta(seconds=end_offset_seconds)
                        
                        # Prepare insert
                        step_json_data = json.dumps(points, ensure_ascii=False)
                        
                        insert_query = text("""
                            INSERT INTO origin.fds_curves_by_step (
                                origin_data_id, step, curve_plain, start_time, end_time,
                                actualprogramid, systemid, startselection, ok_nok_code, 
                                lastexecutedstep, cyclenumber, bsn, progselection, 
                                create_time, update_time
                            ) VALUES (
                                :origin_data_id, :step, CAST(:curve_plain AS jsonb), :start_time, :end_time,
                                :actualprogramid, :systemid, :startselection, :ok_nok_code,
                                :lastexecutedstep, :cyclenumber, :bsn, :progselection,
                                :create_time, :update_time
                            )
                        """)
                        
                        insert_params = {
                            "origin_data_id": record_id,
                            "step": step_num,
                            "curve_plain": step_json_data,
                            "start_time": step_start_time,
                            "end_time": step_end_time,
                            **metadata
                        }
                        
                        conn.execute(insert_query, insert_params)
                        current_point_index += step_points_count

                    success += 1
                    if success % 10 == 0:
                        print(f"  âœ“ å·²æˆåŠŸå¤„ç† {success} æ¡è®°å½•")
                    conn.commit()  # Commit after each record
                    
                except Exception as e:
                    print(f"  âœ— ID {record_id}: é”™è¯¯ - {e}")
                    failed += 1
                    conn.rollback() # Rollback on error
                    continue
            
            # æäº¤æ‰¹æ¬¡ (redundant but keeps logic clean if loop finishes)
            # conn.commit() 
            # print(f"  ğŸ’¾ æ‰¹æ¬¡å·²æäº¤åˆ°æ•°æ®åº“")
            
            processed += len(records)
        
        # æœ€ç»ˆç»Ÿè®¡
        print(f"\n{'='*60}")
        print(f"ğŸ“ˆ å¤„ç†å®Œæˆ!")
        print(f"{'='*60}")
        print(f"æ€»å¤„ç†æ•°: {processed}")
        print(f"âœ“ æˆåŠŸ: {success}")
        print(f"âœ— å¤±è´¥: {failed}")
        print(f"æˆåŠŸç‡: {success/processed*100:.2f}%" if processed > 0 else "N/A")
        print(f"{'='*60}\n")


def main():
    """
    ä¸»å‡½æ•°
    """
    # æ•°æ®åº“é…ç½®
    DB_CONFIG = {
        'host': '10.18.120.240',
        'port': 35432,
        'database': 'miot_quality_data',
        'user': 'postgres',
        'password': '6edef2d746f2274cab951a452d5fc13d',
        'driver': 'pg8000'  # ä½¿ç”¨ pg8000 é©±åŠ¨
    }
    
    # å¤„ç†é…ç½®
    BATCH_SIZE = 100
    START_DATE = '2024-12-01'  # åªå¤„ç†12æœˆåŠä»¥åçš„æ•°æ®
    MIN_AUTOINDEX = 4486968    # åªå¤„ç† autoindex > 2955030 çš„è®°å½•
    
    print("="*60)
    print("FDSæ›²çº¿æ•°æ®åº“è§£æå™¨ (SQLAlchemy)")
    print("="*60)
    print(f"æ•°æ®åº“: {DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}")
    print(f"æ•°æ®è¡¨: origin.fds_curves")
    print(f"èµ·å§‹æ—¥æœŸ: {START_DATE}")
    print(f"æœ€å°autoindex: {MIN_AUTOINDEX}")
    print(f"æ‰¹å¤„ç†å¤§å°: {BATCH_SIZE}")
    print(f"æ•°æ®åº“é©±åŠ¨: {DB_CONFIG['driver']}")
    print("="*60)
    
    try:
        # åˆ›å»ºæ•°æ®åº“å¼•æ“
        engine = create_db_engine(**DB_CONFIG)
        
        # å¤„ç†æ›²çº¿æ•°æ®
        process_curves(engine, batch_size=BATCH_SIZE, start_date=START_DATE, min_autoindex=MIN_AUTOINDEX)
        
        # å…³é—­å¼•æ“
        engine.dispose()
        print("âœ“ æ•°æ®åº“è¿æ¥å·²å…³é—­")
        
    except Exception as e:
        print(f"\nâœ— è‡´å‘½é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
