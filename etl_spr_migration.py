"""
SPR (Self-Piercing Rivet) å·¥è‰ºæ•°æ® ETL è¿ç§»è„šæœ¬
ä» origin.bs_spr_detail å’Œ origin.bs_spr_graph è¿ç§»åˆ° biz ä¸šåŠ¡è¡¨

æ•°æ®å…³è”:
- bs_spr_detail.id = bs_spr_graph.id (ä¸»é”®å…³è”)
- result_sequence_number ä½œä¸ºå¾ªç¯ç¼–å· (cyclenumber)

æ›²çº¿æ•°æ®æ ¼å¼:
- gzip å‹ç¼©çš„ float32 æ•°ç»„
- æ¯ä¸ª result æœ‰ 2 æ¡æ›²çº¿: Force/Time å’Œ Stroke/Time
"""

import struct
import gzip
import json
import argparse
import os
import concurrent.futures
import time
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta

# --- CHECKPOINT FILE ---
CHECKPOINT_FILE = os.path.join(os.path.dirname(__file__), "spr_v2_checkpoint.json")


def load_checkpoint():
    """åŠ è½½æ–­ç‚¹ä¿¡æ¯"""
    if os.path.exists(CHECKPOINT_FILE):
        try:
            with open(CHECKPOINT_FILE, 'r') as f:
                data = json.load(f)
                print(f"ğŸ“Œ åŠ è½½æ–­ç‚¹: autoindex={data.get('last_autoindex')}, æ—¶é—´={data.get('last_time')}")
                return data
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ–­ç‚¹å¤±è´¥: {e}")
    return {"last_autoindex": 0, "last_time": None, "success_count": 0, "fail_count": 0}


def save_checkpoint(autoindex, success_count=0, fail_count=0):
    """ä¿å­˜æ–­ç‚¹ä¿¡æ¯"""
    data = {
        "last_autoindex": autoindex,
        "last_time": datetime.now().isoformat(),
        "success_count": success_count,
        "fail_count": fail_count
    }
    with open(CHECKPOINT_FILE, 'w') as f:
        json.dump(data, f, indent=2)
    print(f"ğŸ’¾ ä¿å­˜æ–­ç‚¹: autoindex={autoindex}")

# --- CONFIGURATION ---
DB_CONFIG = {
    'host': '10.18.120.240',
    'port': 35432,
    'database': 'equipment_mechanism',
    'user': 'postgres',
    'password': '6edef2d746f2274cab951a452d5fc13d',
    'driver': 'pg8000'
}

CRAFT_TYPE = 'SPR'

# --- CURVE PARSING ---
def parse_spr_curve(curve_data):
    """
    è§£æ SPR æ›²çº¿æ•°æ®
    è¾“å…¥: gzip å‹ç¼©çš„äºŒè¿›åˆ¶æ•°æ®
    è¾“å‡º: float32 æ•°ç»„
    """
    if curve_data is None:
        return None
    
    try:
        data = bytes(curve_data)
        decompressed = gzip.decompress(data)
        
        # è§£æä¸º float32 æ•°ç»„
        num_floats = len(decompressed) // 4
        values = struct.unpack(f'<{num_floats}f', decompressed[:num_floats*4])
        
        return [round(v, 4) for v in values]
    except Exception as e:
        print(f"æ›²çº¿è§£æé”™è¯¯: {e}")
        return None


def generate_time_axis(num_points, cycle_time):
    """
    ç”Ÿæˆæ—¶é—´è½´æ•°æ® (X è½´)
    å‡è®¾æ•°æ®ç‚¹å‡åŒ€åˆ†å¸ƒåœ¨ cycle_time å†…
    """
    if num_points <= 1:
        return [0.0]
    
    dt = cycle_time / (num_points - 1) if cycle_time > 0 else 0.001
    return [round(i * dt, 6) for i in range(num_points)]


def create_db_engine():
    conn_str = f"postgresql+{DB_CONFIG['driver']}://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
    # Increase pool size for multi-threading
    return create_engine(conn_str, echo=False, pool_size=20, max_overflow=50)


def migrate_single_record(detail_id, engine):
    """
    è¿ç§»å•æ¡ SPR è®°å½•
    æ³¨æ„ï¼šä½¿ç”¨ engine.begin() ç¡®ä¿çº¿ç¨‹å®‰å…¨å’Œå³æ—¶äº‹åŠ¡å¤„ç†
    """
    # print(f"\nå¤„ç† SPR è®°å½•: id={detail_id}...")
    
    try:
        with engine.begin() as conn:
            # 0. æ£€æŸ¥æ˜¯å¦å·²è¿ç§»
            check_exist = text("SELECT id FROM biz.result WHERE source_id = :sid AND craft_type = :craft")
            exist_res = conn.execute(check_exist, {"sid": detail_id, "craft": CRAFT_TYPE}).fetchone()
            if exist_res:
                # print(f"âš ï¸ è®°å½• id={detail_id} å·²å­˜åœ¨ (result_id={exist_res[0]}), è·³è¿‡")
                return True
            
            # 1. æŸ¥è¯¢ bs_spr_detail è¯¥ id çš„æ‰€æœ‰è¡Œï¼ˆæ¯ä¸ª parameter_type ä¸€è¡Œï¼‰
            query_all_params = text("""
                SELECT DISTINCT id, device_name, result_sequence_number, result_date_time,
                    program_id, p_name, program_identifier, program_version,
                    final_force, final_stroke, start_distance, end_distance,
                    velocity, cycle_time, limit_high, limit_low, parameter_type,
                    short_description, bsn
                FROM origin.bs_spr_detail_v2 WHERE id = :id
            """)
            all_rows = conn.execute(query_all_params, {"id": detail_id}).fetchall()
            
            if not all_rows:
                # print(f"âŒ æœªæ‰¾åˆ° id={detail_id} çš„è®°å½•")
                return False
            
            # print(f"  æ‰¾åˆ° {len(all_rows)} ä¸ª parameter_type è¡Œ")
            
            # ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºåŸºç¡€æ•°æ®ï¼ˆç”¨äº result è¡¨ï¼‰
            first_row = all_rows[0]
            res_id = first_row[0]
            device_name = first_row[1]
            result_seq_num = first_row[2]  # cyclenumber
            result_time = first_row[3]
            program_id_num = first_row[4]
            p_name = first_row[5]
            program_identifier = first_row[6]
            program_version = first_row[7]
            final_force = float(first_row[8]) if first_row[8] else 0.0
            final_stroke = float(first_row[9]) if first_row[9] else 0.0
            start_distance = float(first_row[10]) if first_row[10] else 0.0
            end_distance = float(first_row[11]) if first_row[11] else 0.0
            velocity = float(first_row[12]) if first_row[12] else 0.0
            cycle_time = float(first_row[13]) if first_row[13] else 0.0
            short_desc = first_row[17]
            bsn = first_row[18]
            
            # 2. è®¡ç®— result_status
            if short_desc:
                desc_upper = short_desc.upper()
                if 'NOT' in desc_upper or 'NOK' in desc_upper:
                    result_status = 0
                elif 'OK' in desc_upper:
                    result_status = 1
                else:
                    result_status = 0
            else:
                result_status = 0
            
            # 3. è®¡ç®— end_time
            end_time = result_time + timedelta(seconds=cycle_time) if cycle_time else result_time
            
            # 4. æŸ¥è¯¢å…³è”çš„æ›²çº¿æ•°æ® (é€šè¿‡ id å…³è”)
            query_graphs = text("""
                SELECT id, graph_type, graph_values
                FROM origin.bs_spr_graph_v2 WHERE id = :id
            """)
            graphs = conn.execute(query_graphs, {"id": detail_id}).fetchall()
            
            # print(f"  æ‰¾åˆ° {len(graphs)} æ¡æ›²çº¿è®°å½•")
            
            # A. ä¸ºæ¯ç§ parameter_type æ’å…¥ Program è®°å½•
            insert_prog = text("""
                INSERT INTO biz.program (program_id, version, program_name, craft_type, parameter_type,
                                        device_type, target_value, upper_limit, lower_limit)
                VALUES (:pid, :ver, :pname, :craft, :param_type, :dev, :target, :upper, :lower)
                ON CONFLICT (program_id, version, parameter_type) DO NOTHING
                RETURNING id
            """)
            
            # ä½¿ç”¨å­—å…¸å»é‡ï¼ˆç›¸åŒ parameter_type å¯èƒ½æœ‰é‡å¤è¡Œï¼‰
            param_programs = {}  # parameter_type -> program_db_id
            for row in all_rows:
                param_type = row[16]  # parameter_type
                limit_high = float(row[14]) if row[14] else 0.0
                limit_low = float(row[15]) if row[15] else 0.0
                
                if param_type in param_programs:
                    continue  # å·²å¤„ç†è¿‡è¯¥ parameter_type
                
                target_value = (limit_high + limit_low) / 2 if (limit_high and limit_low) else None
                prog_result = conn.execute(insert_prog, {
                    "pid": program_identifier or str(program_id_num),
                    "ver": str(program_version) if program_version else "1",
                    "pname": p_name,
                    "craft": CRAFT_TYPE,
                    "param_type": param_type,
                    "dev": device_name,
                    "target": target_value,
                    "upper": limit_high,
                    "lower": limit_low
                }).fetchone()
                
                if prog_result:
                    param_programs[param_type] = prog_result[0]
                else:
                    # æŸ¥è¯¢å·²å­˜åœ¨çš„ program
                    fetch_prog = text("SELECT id FROM biz.program WHERE program_id = :pid AND version = :ver AND parameter_type = :param_type")
                    param_programs[param_type] = conn.execute(fetch_prog, {
                        "pid": program_identifier or str(program_id_num),
                        "ver": str(program_version) if program_version else "1",
                        "param_type": param_type
                    }).scalar()
            
            # print(f"  âœ“ æ’å…¥/æ›´æ–° {len(param_programs)} æ¡ program è®°å½•")
            
            # é€‰æ‹©ä¸€ä¸ª program_db_id å…³è”åˆ° resultï¼ˆä½¿ç”¨ Final Force æˆ–ç¬¬ä¸€ä¸ªï¼‰
            program_db_id = param_programs.get('Final Force') or (list(param_programs.values())[0] if param_programs else None)
            
            # B. æ’å…¥ Result (ä½¿ç”¨ source_id å­˜å‚¨åŸå§‹IDï¼Œè®© id è‡ªåŠ¨ç”Ÿæˆ)
            insert_result = text("""
                INSERT INTO biz.result (
                    source_id, cyclenumber, device_name, craft_type, system_id, bsn,
                    program_id, program_ver_id, result_status,
                    start_time, end_time, cycle_time, key_value
                ) VALUES (
                    :source_id, :cnum, :dev, :craft, :sys, :bsn,
                    :pid_str, :pid_fk, :status,
                    :start, :end, :duration, :key_val
                ) RETURNING id
            """)
            result_insert = conn.execute(insert_result, {
                "source_id": res_id,  # åŸå§‹è¡¨çš„ ID
                "cnum": str(result_seq_num),
                "dev": device_name,
                "craft": CRAFT_TYPE,
                "sys": device_name,
                "bsn": bsn,
                "pid_str": program_identifier or str(program_id_num),
                "pid_fk": program_db_id,
                "status": result_status,
                "start": result_time,
                "end": end_time,
                "duration": cycle_time,
                "key_val": final_force
            })
            
            # è·å–è‡ªåŠ¨ç”Ÿæˆçš„ result_id
            result_db_id = result_insert.fetchone()[0]
            # print(f"  ç”Ÿæˆ result_id: {result_db_id}")
            
            # C. æ’å…¥ Extensionï¼ˆå­˜å‚¨æ‰€æœ‰ parameter_type çš„é™å€¼ä¿¡æ¯ï¼‰
            insert_ext = text("""
                INSERT INTO biz.extension (result_id, extra_data)
                VALUES (:rid, CAST(:extra AS jsonb))
            """)
            
            # æ”¶é›†æ‰€æœ‰ parameter_type çš„é™å€¼
            param_limits = {}
            for row in all_rows:
                param_type = row[16]
                if param_type and param_type not in param_limits:
                    param_limits[param_type] = {
                        "limit_high": float(row[14]) if row[14] else None,
                        "limit_low": float(row[15]) if row[15] else None
                    }
            
            extra_data = {
                "final_force": final_force,
                "final_stroke": final_stroke,
                "start_distance": start_distance,
                "end_distance": end_distance,
                "velocity": velocity,
                "parameter_limits": param_limits  # æ‰€æœ‰ parameter_type çš„é™å€¼
            }
            conn.execute(insert_ext, {"rid": result_db_id, "extra": json.dumps(extra_data)})
            
            # D. æ’å…¥ Step (SPR åªæœ‰ä¸€ä¸ªæ­¥éª¤)
            insert_step = text("""
                INSERT INTO biz.step (
                    result_id, step_index, step_name, step_result,
                    step_value, target_value, start_time, end_time
                ) VALUES (
                    :rid, :sidx, :sname, :sres,
                    :sval, :target, :sstart, :send
                ) RETURNING id
            """)
            step_result = conn.execute(insert_step, {
                "rid": result_db_id,  # ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ result_id
                "sidx": 0,
                "sname": "Riveting",
                "sres": result_status,
                "sval": final_force,
                "target": target_value,
                "sstart": result_time,
                "send": end_time
            })
            step_db_id = step_result.fetchone()[0]
            
            # E. æ’å…¥ Curves
            insert_curve = text("""
                INSERT INTO biz.curve (result_id, step, curve_type, start_time, end_time, data_points)
                VALUES (:rid, :step, :ctype, :sstart, :send, CAST(:data AS jsonb))
            """)
            
            for graph in graphs:
                graph_type = graph[1]  # Force/Time or Stroke/Time
                graph_values = graph[2]
                
                if graph_values:
                    values = parse_spr_curve(graph_values)
                    if values:
                        # ç”Ÿæˆæ—¶é—´è½´
                        time_axis = generate_time_axis(len(values), cycle_time)
                        
                        # ç¡®å®š curve_type
                        if 'Force' in graph_type:
                            curve_type = 'FORCE'
                        elif 'Stroke' in graph_type:
                            curve_type = 'STROKE'
                        else:
                            curve_type = graph_type.upper().replace('/', '_')
                        
                        payload = {"x": time_axis, "y": values}
                        conn.execute(insert_curve, {
                            "rid": result_db_id,  # ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ result_id
                            "step": 0,
                            "ctype": curve_type,
                            "sstart": result_time,
                            "send": end_time,
                            "data": json.dumps(payload)
                        })
                        # print(f"  âœ“ æ›²çº¿ {curve_type}: {len(values)} ä¸ªæ•°æ®ç‚¹")
            
            # F. æ’å…¥ Alarm (å¦‚æœ NOK)
            if result_status == 0:
                insert_alarm = text("""
                    INSERT INTO biz.alarm (result_id, step_id, alarm_code, alarm_level, alarm_msg, device_id)
                    VALUES (:rid, :sid, :code, :level, :msg, :dev)
                """)
                conn.execute(insert_alarm, {
                    "rid": result_db_id,  # ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ result_id
                    "sid": step_db_id,
                    "code": "SPR_NOK",
                    "level": "ERROR",
                    "msg": short_desc or "SPR process failed",
                    "dev": device_name
                })
                # print(f"  âœ“ æŠ¥è­¦å·²è®°å½•")
            
            # print(f"âœ… æˆåŠŸè¿ç§» id={detail_id}")
            return True
        
    except Exception as e:
        # import traceback
        # traceback.print_exc()
        # print(f"âŒ è¿ç§»å¤±è´¥ id={detail_id}: {e}")
        return False


def migrate_batch(start_autoindex=None, batch_size=200, limit=None, resume=False, workers=5):
    """
    æ‰¹é‡å¤šçº¿ç¨‹è¿ç§» SPR è®°å½•
    """
    engine = create_db_engine()
    
    # å¤„ç†æ–­ç‚¹ç»­ä¼ 
    checkpoint = load_checkpoint()
    if resume and checkpoint.get("last_autoindex"):
        start_autoindex = checkpoint["last_autoindex"]
        print(f"ğŸ“Œ ä»æ–­ç‚¹æ¢å¤: autoindex > {start_autoindex}")
    elif start_autoindex:
        print(f"ğŸš€ ä»æŒ‡å®šä½ç½®å¼€å§‹: autoindex > {start_autoindex}")
    
    success = checkpoint.get("success_count", 0) if resume else 0
    failed = checkpoint.get("fail_count", 0) if resume else 0
    last_autoindex = start_autoindex or 0
    total_processed = 0
    
    while True:
        if limit and total_processed >= limit:
            break

        # FETCH BATCH
        with engine.connect() as conn:
            query = """
                SELECT sid, id 
                FROM origin.bs_spr_detail_v2
                WHERE sid > :last_idx
                ORDER BY sid
                LIMIT :batch
            """
            rows = conn.execute(text(query), {"last_idx": last_autoindex, "batch": batch_size}).fetchall()
        
        if not rows:
            print("æ²¡æœ‰æ›´å¤šæ•°æ®äº†ã€‚")
            break
        
        # å»é‡ï¼šåŒä¸€ä¸ª id å¯èƒ½æœ‰å¤šè¡Œ(ä¸åŒ parameter_type)ï¼Œåªå–ç¬¬ä¸€ä¸ªé‡åˆ°çš„
        seen_ids = set()
        unique_tasks = [] # (sid, id)
        
        for row in rows:
            sid, detail_id = row[0], row[1]
            if detail_id not in seen_ids:
                seen_ids.add(detail_id)
                unique_tasks.append((sid, detail_id))
            # Even if duplicate ID (and we skip processing), we need to acknowledge the sid was "seen"
            # The last SID in the batch is what we care about for checkpointing.
            # But wait, we iterate by SID. duplicate rows have different SIDs?
            # Yes, bs_spr_detail_v2 has diff IDs because of diff parameter_type? 
            # Or one 'result' has multiple 'rows'. Each row has unique ID?
            # 'id' is distinct ID. 'sid' is sequence?
            # Actually, `id` (UUID?) is what we migrate. `sid` is just for iteration.
            # So duplicates are rows with same `id`.
        
        print(f"ğŸ“¥ è·å–æ‰¹æ¬¡: {len(rows)} è¡Œ -> {len(unique_tasks)} ä¸ªç‹¬ç«‹ä»»åŠ¡ (Start SID: {rows[0][0]}, End SID: {rows[-1][0]})")

        # PARALLEL EXECUTION
        batch_success = 0
        batch_failed = 0
        max_sid_in_batch = last_autoindex
        
        start_time = time.time()

        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            # Map future -> (sid, detail_id)
            future_to_task = {executor.submit(migrate_single_record, task[1], engine): task for task in unique_tasks}
            
            for future in concurrent.futures.as_completed(future_to_task):
                sid, detail_id = future_to_task[future]
                try:
                    result = future.result()
                    if result:
                        batch_success += 1
                    else:
                        batch_failed += 1
                except Exception as exc:
                    print(f"  ğŸ’¥ {detail_id} generated an exception: {exc}")
                    batch_failed += 1
                
        # Update checkpoint to the last SID in the fetch, regardless of whether it was unique or duplicate
        # because we have "processed" up to that point.
        max_sid_in_batch = rows[-1][0] 
        
        end_time = time.time()
        duration = end_time - start_time
        speed = len(rows) / duration if duration > 0 else 0
        
        success += batch_success
        failed += batch_failed
        total_processed += len(rows)
        last_autoindex = max_sid_in_batch
        
        # SAVE CHECKPOINT
        save_checkpoint(last_autoindex, success, failed)
        print(f"â±ï¸ æ‰¹æ¬¡å®Œæˆ. ç”¨æ—¶: {duration:.2f}s, é€Ÿåº¦: {speed:.1f} rows/s. è¿›åº¦: æ€»æˆåŠŸ {success}, æ€»å¤±è´¥ {failed}, æœ€æ–°æ–­ç‚¹ {last_autoindex}")


def main():
    parser = argparse.ArgumentParser(description="SPR ETL Migration")
    parser.add_argument("--single-id", type=int, help="è¿ç§»å•æ¡è®°å½• (æµ‹è¯•ç”¨)")
    parser.add_argument("--batch", action="store_true", help="æ‰¹é‡è¿ç§»æ¨¡å¼")
    parser.add_argument("--resume", action="store_true", help="ä»ä¸Šæ¬¡æ–­ç‚¹ç»§ç»­")
    parser.add_argument("--start-id", type=int, help="æ‰¹é‡æ¨¡å¼èµ·å§‹ ID")
    parser.add_argument("--limit", type=int, help="æ‰¹é‡æ¨¡å¼æœ€å¤§è®°å½•æ•°")
    parser.add_argument("--workers", type=int, default=10, help="å¹¶å‘çº¿ç¨‹æ•°") # Default 10 workers
    args = parser.parse_args()
    
    engine = create_db_engine()
    
    if args.single_id:
        try:
             # Just pass engine, migrate_single_record now takes engine
             # Wait, args.single_id assumes `id` is integer? In spr table it might be uuid or int?
             # Based on SQL `id = :id` it seems generic.
             # The table definition implies `id` might be int or uuid. 
             # `migrate_single_record` arg name `detail_id` suggests ID.
             # The CLI says type=int. If it is UUID, this will fail.
             # But let's assume it works as previous script used it.
             migrate_single_record(args.single_id, engine)
        except Exception as e:
            print(f"äº‹åŠ¡å¤±è´¥: {e}")
    elif args.batch:
        migrate_batch(
            start_autoindex=args.start_id, 
            limit=args.limit, 
            resume=args.resume,
            workers=args.workers
        )
    else:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  --single-id <id>  : æµ‹è¯•å•æ¡è¿ç§»")
        print("  --batch           : æ‰¹é‡è¿ç§»æ¨¡å¼")
        print("  --resume          : æ¢å¤æ¨¡å¼")
        print("  --workers <n>     : çº¿ç¨‹æ•° (é»˜è®¤10)")


if __name__ == "__main__":
    main()
